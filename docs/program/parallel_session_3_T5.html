<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Parallel Session 3: T5 – CERE2025 - 10th Conference of the Consortium of European Research on Emotion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../images/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-3b2160a2fc92221658aa6ab2fa9f3c79.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link href="../site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="../site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="../site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="../site_libs/datatables-binding-0.33/datatables.js"></script>
<script src="../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="../site_libs/dt-ext-responsive-1.13.6/css/responsive.dataTables.min.css" rel="stylesheet">
<script src="../site_libs/dt-ext-responsive-1.13.6/js/dataTables.responsive.min.js"></script>
<link href="../site_libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="../site_libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="../site_libs/dt-ext-rowgroup-1.13.6/css/rowGroup.dataTables.min.css" rel="stylesheet">
<script src="../site_libs/dt-ext-rowgroup-1.13.6/js/dataTables.rowGroup.min.js"></script>


<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logoCERE2025.png" alt="" class="navbar-logo">
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-program" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Program</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-program">    
        <li>
    <a class="dropdown-item" href="../program/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../program/days/july_16.html">
 <span class="dropdown-text">July 16</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../program/days/july_17.html">
 <span class="dropdown-text">July 17</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../program/days/july_18.html">
 <span class="dropdown-text">July 18</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-keynotes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Keynotes</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-keynotes">    
        <li>
    <a class="dropdown-item" href="../keynotes/agnes_moors.html">
 <span class="dropdown-text">Prof.&nbsp;Agnes Moors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../keynotes/jose-miguel_fernandez-dols.html">
 <span class="dropdown-text">Prof.&nbsp;José-Miguel Fernández-Dols</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../keynotes/steven_heine.html">
 <span class="dropdown-text">Prof.&nbsp;Steve Heine</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../submission/index.html"> 
<span class="menu-text">Instructions for Presenters</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-registration" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Registration</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-registration">    
        <li>
    <a class="dropdown-item" href="../attend/registration.html">
 <span class="dropdown-text">Registration Information</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://uga.azur-colloque.fr/inscription/en/232/inscription" target="_blank">
 <span class="dropdown-text">Log In the Registration Portal</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-attend" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Attend</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-attend">    
        <li>
    <a class="dropdown-item" href="../attend/venue.html">
 <span class="dropdown-text">Venue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../attend/accommodation.html">
 <span class="dropdown-text">Accommodation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../attend/social.html">
 <span class="dropdown-text">Social Events</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../attend/transportation.html">
 <span class="dropdown-text">Transportation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-people" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">People</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-people">    
        <li>
    <a class="dropdown-item" href="../people/index.html">
 <span class="dropdown-text">Organising Committee</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../people/committee.html">
 <span class="dropdown-text">Scientific Committee</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../faq.html"> 
<span class="menu-text">FAQ</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://x.com/CERE_Emotion" title="" class="quarto-navigation-tool px-1" aria-label="" target="_blank"><i class="bi bi-twitter-x"></i></a>
    <a href="https://www.instagram.com/CERE_Emotion/" title="" class="quarto-navigation-tool px-1" aria-label="" target="_blank"><i class="bi bi-instagram"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <div id="quarto-announcement" data-announcement-id="a099f15fa5df9b7c51d563318a7a5b0d" class="alert alert-info hidden"><i class="bi bi-info-circle quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>A new version of the program is available <a href="https://www.cere2025.com/program/">here</a> (July 4th)</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Parallel Session 3: T5</h1>
</div>



<div class="quarto-title-meta column-page">

    
  
    
  </div>
  


</header>


<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-dd63798d2d52c8fcd5ce" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-dd63798d2d52c8fcd5ce">{"x":{"filter":"none","vertical":false,"extensions":["Responsive"],"data":[["Session","Track","Time","Type","Title","Abstract"],["Parallel Session 3","T5","16:20  -  17:20","Symposium","Research Methods and Materials for Studying Emotions in the Face - Chair: Dennis Küster","From psychology to affective computing and the design of virtual humans, the face is believed to provide a key window for understanding and eliciting emotions and social signals. This symposium presents new methods and materials for emotion researchers focusing on the face. By integrating explainable AI with deep learning models, the first talk reveals critical discrepancies where AI-based systems deviate from Ekman's neurocultural model. It underscores the need for hybrid models including traditional symbolic AI to help bridge the gap between data-driven and expert-informed analysis. In the second talk, electromyography-based action unit recognition is used to refine facial expression measurement, offering high accuracy while enabling privacy-preserving applications and real-time virtual human animation. The use of hyper-realistic virtual humans, in the third talk, expands research possibilities by moving beyond static images and prototypical emotions, offering dynamic and interactive stimuli to study more nuanced affective states. Complementing this, a newly developed database of dynamic emotional crying behaviours (talk 4) fills a critical gap by systematically varying not only facial expressions but also tears, vocalizations, and gestures, providing a more holistic perspective on how emotions are communicated through the face. Finally, in talk number five, a critical assessment of automatic facial landmark detection highlights biases and systematic distortions across existing emotion recognition systems, emphasizing the need for improved precision in AI-driven affect analysis. Together, these studies aim to advance our methodological toolkit for emotion research by fostering greater accuracy, interpretability, and ecological validity in the study of facial expressions, tearful displays, and facial communication."]],"container":"<table class=\"display\">\n  <thead>\n    <tr><\/tr>\n  <\/thead>\n<\/table>","options":{"responsive":{"details":{"target":1}},"dom":"t","pageLength":-1,"ordering":false,"columnDefs":[{"name":"rowname","targets":0},{"name":"V1","targets":1}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[-1,10,25,50,100],"rowCallback":"function(row, data, displayNum, displayIndex, dataIndex) {\nvar value=data[0]; $(this.api().cell(row, 0).node()).css({'font-weight':'bold','color':'#eeeeee','background-color':'#E84E0F'});\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
</div>
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-a48e5e4ee49a99f4f1df" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-a48e5e4ee49a99f4f1df">{"x":{"filter":"none","vertical":false,"extensions":["Responsive","RowGroup"],"fillContainer":false,"data":[["Jens Gebele - Do AI and Humans Look at Emotions the Same Way? A Study Using Explainable A","Jens Gebele - Do AI and Humans Look at Emotions the Same Way? A Study Using Explainable A","Dennis Küster - EMG-based Action Unit Recognition and Animation","Dennis Küster - EMG-based Action Unit Recognition and Animation","Nick van Apeldoorn - From Altered Photographs to Virtual Humans: Exploring Emotional Perception in Experimental Research","Nick van Apeldoorn - From Altered Photographs to Virtual Humans: Exploring Emotional Perception in Experimental Research","Monika Wróbel - The Dynamic Posed Emotional Crying Behavior Database (DPECBD): A Comprehensive Resource to Study the Multifaceted Nature of Emotional Crying","Monika Wróbel - The Dynamic Posed Emotional Crying Behavior Database (DPECBD): A Comprehensive Resource to Study the Multifaceted Nature of Emotional Crying","Axel Zinkernagel - Not just points in a cloud: A marker-based comparison of biases in dynamic landmark detection performance across four automatic emotion recognition systems","Axel Zinkernagel - Not just points in a cloud: A marker-based comparison of biases in dynamic landmark detection performance across four automatic emotion recognition systems"],["Time","Abstract","Time","Abstract","Time","Abstract","Time","Abstract","Time","Abstract"],["16:20","Despite the growing capabilities of Deep Learning (DL) models in Facial Emotion Recognition (FER), their decision-making processes remain opaque, raising questions about how well they align with human expert perception. This study investigates whether AI systems and human experts focus on the same facial regions when observing facial behavior to interpret emotions. We focus on three distinct emotions - happiness, which achieves the highest recognition accuracy in DL models trained on benchmark datasets; disgust, which exhibits the lowest accuracy; and sadness, the semantic opposite of happiness. To explore how AI models interpret facial expressions, we use two explainable AI (XAI) methods - Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive Explanations (SHAP). Both techniques create heatmaps highlighting which facial areas are most relevant for the model's decision. However, they achieve this in different ways: Grad-CAM (model-specific) highlights the regions of an image that most influence the AI's decision by analyzing how the features learned by the network are activated. SHAP (model-agnostic) evaluates how changes to different image regions affect the model's prediction, assigning importance scores to each area. By applying these methods to DL models trained on three established research datasets - FER-2013, RAF-DB, and AffectNet - we visualize the facial regions identified by the models. We then analyze how these regions align with expert-defined facial Action Units (AUs) from the Facial Action Coding System (FACS). Our findings reveal discrepancies, highlighting areas where AI systems deviate from Ekman's neurocultural theory of emotion. As a step forward, we propose combining DL models with traditional symbolic AI approaches that encode expert knowledge to bridge these gaps between AI and expert-based emotion recognition. This hybrid approach could improve the interpretability and consistency of FER systems. Symposium Talk 2: EMG-based Action Unit Recognition and Animation The human ability to convey emotions and communicate with others through facial expressions is essential for everyday social interactions. This system is highly complex and responsive to social context, making us acutely aware when our expressions are observed or evaluated. Impairments, whether in virtual reality, when wearing a facial mask, or due to conditions such as Parkinson's disease, may therefore hinder communication unless compensated for. Facial electromyography (EMG) and the Facial Action Coding System (FACS) have long been gold standards for studying facial expressions. EMG excels in detecting subtle muscle activity with high temporal resolution, while FACS provides structured expert coding. Automated action unit recognition (AUR) has advanced video-based FACS analysis, yet classifying AUs using EMG remains underexplored. We present findings from four studies applying machine and deep learning models to classify AUs from facial EMG data. A pilot study demonstrated near-perfect recognition (&gt; .95) for four peak AUs (AU1, AU2, AU4, AU9) in an expert-trained participant. A second study extended this to eight AUs (adding AU12, AU17, AU20, AU24) in a multi-user setting (N = 32), achieving 82.5% accuracy. A third study (N = 4) explored peak vs. subtle expressions across 16 classes plus neutral, showing comparable accuracies for both and suggesting smaller electrodes enhance subtle expression detection. An ongoing fourth study (N = 4) further investigates subtle expressions and user-independent models. We discuss how EMG-based AUR can contribute to privacy-preserving facial expression research across diverse applications. Finally, we present a use case demonstrating how EMG-based AUR can animate virtual humans in virtual reality environments. Symposium Talk 3: From Altered Photographs to Virtual Humans: Exploring Emotional Perception in Experimental Research In experimental emotion research, such as the social context of mimicry, crying, or empathy, standard paradigms typically use altered still images. For example, in crying research, tears are edited in or out, as dynamic materials have largely been lacking. Simple images have clear advantages: they are easier to manipulate and integrate into various experimental scenarios. Pre-recorded videos, however, may offer stronger stimuli, allowing systematic variation of multimodal and behavioural cues such as vocalizations and gestures. Likewise, dynamic materials may also be better suited for studying genuine, complex, or profound emotional displays. However, video stimuli also face several limitations - they are resource-intensive to produce, limited in scope, and harder to integrate believably across contexts. Here, hyper-realistic virtual humans (VHs) open new possibilities, especially for real-time and interactive experimental research, combining the benefits of still images and dynamic materials to create well-controlled stimuli for virtually any context. However, previous VH designs have often relied on outdated emotion theories, focusing on prototypical emotions (Dyck et al., 2008; Joyal et al., 2014; Kätsyri &amp; Sams, 2008) while largely ignoring more complex emotions or profound emotions. In this contribution, we will present preliminary results of the VHESPER project, which explores how VHs may portray complex emotions and how context may modulate the extent to which humans attribute profound emotions to VHs. We conclude by discussing how the design of VHs may learn from and stimulate further research on emotions. Symposium Talk 4: The Dynamic Posed Emotional Crying Behavior Database (DPECBD): A Comprehensive Resource to Study the Multifaceted Nature of Emotional Crying Emotional crying is a complex and multifaceted expression that is frequently observed in humans. Its communicative effects have been recently studied in more detail. However, many studies focus on just one specific feature of emotional crying, most often emotional tears, neglecting the fact that they most commonly occur in combination with other features, such as facial expressions, vocalizations, gestures, and varied temporal dynamics. This research gap is mostly explained by the lack of adequately controlled stimuli depicting different crying features. Here, we provide a solution to this problem by introducing the Dynamic Posed Emotional Crying Behavior Database (DPECBD), an openly available resource of 500 videos depicting 10 actors showing variations in tears intensity, facial expression intensity, vocalizations, gestures, temporal dynamics, and the combination thereof. We present two studies (N = 2729) providing evidence for the validity of the database. In addition, we developed a static supplementary resource (DPECBD-S) with 70 pictures depicting variations in tears and facial expression intensity that was successfully validated across two studies (N = 601). Overall, our findings support the validity of this new stimulus set that closes a gap in the research on the interpersonal effects of emotional crying. Symposium Talk 5: Not just points in a cloud: A marker-based comparison of biases in dynamic landmark detection performance across four automatic emotion recognition systems Automatic facial emotion recognition (AFER) algorithms are now widely implemented in both commercial and open-source software to detect human emotional expressions in images and videos without a need for costly manual annotation. Despite their growing prominence, AFER approaches nevertheless still encounter substantial challenges already at the stage of early output measures, such as accurate landmark detection. The present study (N = 18) conducted a comparative analysis of four AFER systems (Affectiva, Facereader, FACET, Openface) on videos of spontaneous facial expressions of 18 subjects using a point distribution model (PDM) to estimate facial landmark movement. We then compared these AFER-landmarks with the blenderFace method (Zinkernagel et al., 2019), a non-AI-based system for precise marker-based facial expression assessment. To control for any potential degradation of the video quality due to the markers, participants were recorded using both, a standard webcam for AI-based systems, and an ultraviolet-sensitive webcam with sunscreen-painted markers for blenderFace. In contrast to the tested AFER-systems, which use machine learning to estimate landmark coordinates,the blenderFace method directly utilizes markers painted painted by an expert on participants' faces to provide a precise ground truth measure for the landmark coordinates. Our results suggest that the use of AFER-based PDMs mayintroduce substantial artifacts in the measurement of facial landmark movement. Examples include distortionsof the entire PDM due to eye blinks and large mouth movements. Additionally, due to interpolation, PDM-measured facial landmarks are distorted akin to a moving average, particularly on the horizontal axes. We discuss how these kinds of distortions may impede later analyses stages, such as the recognition of asymmetric expressions, action units, or discrete emotional expressions.. Overall, our findings suggest that invisible painted landmarks may be able to shed more light on limitations of popular AFER-systems, and how they might be improved to provide more explainable outputs across different levels of analyses – from landmarks to discrete emotional expressions.","16:35","The human ability to convey emotions and communicate with others through facial expressions is essential for everyday social interactions. This system is highly complex and responsive to social context, making us acutely aware when our expressions are observed or evaluated. Impairments, whether in virtual reality, when wearing a facial mask, or due to conditions such as Parkinson's disease, may therefore hinder communication unless compensated for. Facial electromyography (EMG) and the Facial Action Coding System (FACS) have long been gold standards for studying facial expressions. EMG excels in detecting subtle muscle activity with high temporal resolution, while FACS provides structured expert coding. Automated action unit recognition (AUR) has advanced video-based FACS analysis, yet classifying AUs using EMG remains underexplored. We present findings from four studies applying machine and deep learning models to classify AUs from facial EMG data. A pilot study demonstrated near-perfect recognition (&gt; .95) for four peak AUs (AU1, AU2, AU4, AU9) in an expert-trained participant. A second study extended this to eight AUs (adding AU12, AU17, AU20, AU24) in a multi-user setting (N = 32), achieving 82.5% accuracy. A third study (N = 4) explored peak vs. subtle expressions across 16 classes plus neutral, showing comparable accuracies for both and suggesting smaller electrodes enhance subtle expression detection. An ongoing fourth study (N = 4) further investigates subtle expressions and user-independent models. We discuss how EMG-based AUR can contribute to privacy-preserving facial expression research across diverse applications. Finally, we present a use case demonstrating how EMG-based AUR can animate virtual humans in virtual reality environments.","16:50","In experimental emotion research, such as the social context of mimicry, crying, or empathy, standard paradigms typically use altered still images. For example, in crying research, tears are edited in or out, as dynamic materials have largely been lacking. Simple images have clear advantages: they are easier to manipulate and integrate into various experimental scenarios. Pre-recorded videos, however, may offer stronger stimuli, allowing systematic variation of multimodal and behavioural cues such as vocalizations and gestures. Likewise, dynamic materials may also be better suited for studying genuine, complex, or profound emotional displays. However, video stimuli also face several limitations - they are resource-intensive to produce, limited in scope, and harder to integrate believably across contexts. Here, hyper-realistic virtual humans (VHs) open new possibilities, especially for real-time and interactive experimental research, combining the benefits of still images and dynamic materials to create well-controlled stimuli for virtually any context. However, previous VH designs have often relied on outdated emotion theories, focusing on prototypical emotions (Dyck et al., 2008; Joyal et al., 2014; Kätsyri &amp; Sams, 2008) while largely ignoring more complex emotions or profound emotions. In this contribution, we will present preliminary results of the VHESPER project, which explores how VHs may portray complex emotions and how context may modulate the extent to which humans attribute profound emotions to VHs. We conclude by discussing how the design of VHs may learn from and stimulate further research on emotions.","17:05","Emotional crying is a complex and multifaceted expression that is frequently observed in humans. Its communicative effects have been recently studied in more detail. However, many studies focus on just one specific feature of emotional crying, most often emotional tears, neglecting the fact that they most commonly occur in combination with other features, such as facial expressions, vocalizations, gestures, and varied temporal dynamics. This research gap is mostly explained by the lack of adequately controlled stimuli depicting different crying features. Here, we provide a solution to this problem by introducing the Dynamic Posed Emotional Crying Behavior Database (DPECBD), an openly available resource of 500 videos depicting 10 actors showing variations in tears intensity, facial expression intensity, vocalizations, gestures, temporal dynamics, and the combination thereof. We present two studies (N = 2729) providing evidence for the validity of the database. In addition, we developed a static supplementary resource (DPECBD-S) with 70 pictures depicting variations in tears and facial expression intensity that was successfully validated across two studies (N = 601). Overall, our findings support the validity of this new stimulus set that closes a gap in the research on the interpersonal effects of emotional crying.","17:20","Automatic facial emotion recognition (AFER) algorithms are now widely implemented in both commercial and open-source software to detect human emotional expressions in images and videos without a need for costly manual annotation. Despite their growing prominence, AFER approaches nevertheless still encounter substantial challenges already at the stage of early output measures, such as accurate landmark detection. The present study (N = 18) conducted a comparative analysis of four AFER systems (Affectiva, Facereader, FACET, Openface) on videos of spontaneous facial expressions of 18 subjects using a point distribution model (PDM) to estimate facial landmark movement. We then compared these AFER-landmarks with the blenderFace method (Zinkernagel et al., 2019), a non-AI-based system for precise marker-based facial expression assessment. To control for any potential degradation of the video quality due to the markers, participants were recorded using both, a standard webcam for AI-based systems, and an ultraviolet-sensitive webcam with sunscreen-painted markers for blenderFace. In contrast to the tested AFER-systems, which use machine learning to estimate landmark coordinates,the blenderFace method directly utilizes markers painted painted by an expert on participants' faces to provide a precise ground truth measure for the landmark coordinates. Our results suggest that the use of AFER-based PDMs mayintroduce substantial artifacts in the measurement of facial landmark movement. Examples include distortionsof the entire PDM due to eye blinks and large mouth movements. Additionally, due to interpolation, PDM-measured facial landmarks are distorted akin to a moving average, particularly on the horizontal axes. We discuss how these kinds of distortions may impede later analyses stages, such as the recognition of asymmetric expressions, action units, or discrete emotional expressions.. Overall, our findings suggest that invisible painted landmarks may be able to shed more light on limitations of popular AFER-systems, and how they might be improved to provide more explainable outputs across different levels of analyses – from landmarks to discrete emotional expressions."]],"container":"<table class=\"display\">\n  <thead>\n    <tr><\/tr>\n  <\/thead>\n<\/table>","options":{"scrollX":true,"responsive":{"details":{"target":[1,2]}},"dom":"t","pageLength":-1,"ordering":false,"rowGroup":{"dataSrc":0},"columnDefs":[{"width":"50px","targets":1},{"width":"300px","targets":2},{"visible":false,"targets":0},{"name":"Communication","targets":0},{"name":"name","targets":1},{"name":"value","targets":2}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[-1,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/www\.cere2025\.com\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><img src="https://www.univ-grenoble-alpes.fr/uas/SITEUI/UGA_LOGO_PAGE_INTERIEURE/logo_epe_blanc_sans_marges.svg" class="img-fluid" width="150"></p>
</div>   
    <div class="nav-footer-center">
<p>#CERE2025<br> Université Grenoble Alpes, July 16-18, 2025<br></p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.isre.org/mpage/cere">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/CERE_Emotion">
      <i class="bi bi-twitter-x" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/CERE_Emotion/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>